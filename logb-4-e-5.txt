nohup: ignoring input
WARNING:tensorflow:From /data3T/zml/experiment/lw-eg-recurren-bilater-distill/monodepth_model.py:238: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
/data3T/users/zml/.conda/envs/leftmono/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
2020-04-10 11:51:52.006896: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-04-10 11:51:52.456720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:05:00.0
totalMemory: 10.92GiB freeMemory: 10.77GiB
2020-04-10 11:51:52.456757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)
total number of samples: 29000
total number of steps: 36250
============================
2
number of trainable parameters: 23747176
batch    100 | examples/s: 9.51 | loss: 1.10070 | time elapsed: 0.02h | time left: 7.39h
batch    200 | examples/s: 9.50 | loss: 1.12220 | time elapsed: 0.03h | time left: 6.04h
batch    300 | examples/s: 9.49 | loss: 1.09003 | time elapsed: 0.05h | time left: 5.42h
batch    400 | examples/s: 9.34 | loss: 1.37377 | time elapsed: 0.06h | time left: 5.11h
batch    500 | examples/s: 9.47 | loss: 0.88805 | time elapsed: 0.07h | time left: 4.93h
batch    600 | examples/s: 9.28 | loss: 1.35276 | time elapsed: 0.08h | time left: 4.83h
batch    700 | examples/s: 9.23 | loss: 1.26263 | time elapsed: 0.09h | time left: 4.73h
batch    800 | examples/s: 9.49 | loss: 1.36921 | time elapsed: 0.10h | time left: 4.65h
batch    900 | examples/s: 9.50 | loss: 0.91570 | time elapsed: 0.12h | time left: 4.59h
batch   1000 | examples/s: 9.50 | loss: 1.20746 | time elapsed: 0.13h | time left: 4.53h
batch   1100 | examples/s: 9.53 | loss: 1.14513 | time elapsed: 0.14h | time left: 4.50h
batch   1200 | examples/s: 9.37 | loss: 1.12913 | time elapsed: 0.15h | time left: 4.46h
batch   1300 | examples/s: 9.48 | loss: 0.87355 | time elapsed: 0.16h | time left: 4.42h
batch   1400 | examples/s: 9.55 | loss: 1.07127 | time elapsed: 0.18h | time left: 4.39h
batch   1500 | examples/s: 9.54 | loss: 1.02525 | time elapsed: 0.19h | time left: 4.36h
batch   1600 | examples/s: 9.69 | loss: 1.54864 | time elapsed: 0.20h | time left: 4.34h
batch   1700 | examples/s: 9.05 | loss: 1.05246 | time elapsed: 0.21h | time left: 4.32h
batch   1800 | examples/s: 9.56 | loss: 1.03919 | time elapsed: 0.22h | time left: 4.29h
batch   1900 | examples/s: 9.39 | loss: 1.23472 | time elapsed: 0.24h | time left: 4.27h
batch   2000 | examples/s: 9.69 | loss: 0.95183 | time elapsed: 0.25h | time left: 4.24h
batch   2100 | examples/s: 9.51 | loss: 0.89059 | time elapsed: 0.26h | time left: 4.23h
batch   2200 | examples/s: 9.61 | loss: 0.93830 | time elapsed: 0.27h | time left: 4.21h
batch   2300 | examples/s: 9.52 | loss: 0.97746 | time elapsed: 0.28h | time left: 4.19h
batch   2400 | examples/s: 9.34 | loss: 1.10015 | time elapsed: 0.30h | time left: 4.17h
batch   2500 | examples/s: 9.52 | loss: 0.90001 | time elapsed: 0.31h | time left: 4.15h
batch   2600 | examples/s: 9.66 | loss: 1.11346 | time elapsed: 0.32h | time left: 4.14h
batch   2700 | examples/s: 9.51 | loss: 1.29129 | time elapsed: 0.33h | time left: 4.12h
batch   2800 | examples/s: 9.58 | loss: 1.09227 | time elapsed: 0.34h | time left: 4.10h
batch   2900 | examples/s: 9.57 | loss: 1.24967 | time elapsed: 0.36h | time left: 4.09h
batch   3000 | examples/s: 9.43 | loss: 1.65349 | time elapsed: 0.37h | time left: 4.07h
batch   3100 | examples/s: 9.46 | loss: 0.99244 | time elapsed: 0.38h | time left: 4.06h
batch   3200 | examples/s: 9.40 | loss: 0.99374 | time elapsed: 0.39h | time left: 4.04h
batch   3300 | examples/s: 9.59 | loss: 1.32202 | time elapsed: 0.40h | time left: 4.03h
batch   3400 | examples/s: 9.19 | loss: 1.00754 | time elapsed: 0.41h | time left: 4.01h
batch   3500 | examples/s: 9.33 | loss: 1.07427 | time elapsed: 0.43h | time left: 3.99h
batch   3600 | examples/s: 9.47 | loss: 1.20583 | time elapsed: 0.44h | time left: 3.98h
batch   3700 | examples/s: 9.62 | loss: 0.91838 | time elapsed: 0.45h | time left: 3.97h
batch   3800 | examples/s: 9.59 | loss: 1.94156 | time elapsed: 0.46h | time left: 3.95h
batch   3900 | examples/s: 9.68 | loss: 0.88290 | time elapsed: 0.47h | time left: 3.94h
batch   4000 | examples/s: 9.31 | loss: 0.86030 | time elapsed: 0.49h | time left: 3.92h
batch   4100 | examples/s: 9.50 | loss: 1.20993 | time elapsed: 0.50h | time left: 3.91h
batch   4200 | examples/s: 9.45 | loss: 0.84415 | time elapsed: 0.51h | time left: 3.90h
batch   4300 | examples/s: 9.42 | loss: 0.85671 | time elapsed: 0.52h | time left: 3.88h
batch   4400 | examples/s: 9.25 | loss: 1.23770 | time elapsed: 0.53h | time left: 3.87h
batch   4500 | examples/s: 9.46 | loss: 1.15488 